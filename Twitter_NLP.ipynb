{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install snscrape\n",
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMdskYnxx2GD",
        "outputId": "4bf4a698-d812-4e61-ec61-58d355ec8ec1"
      },
      "id": "OMdskYnxx2GD",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: snscrape in /usr/local/lib/python3.7/dist-packages (0.3.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from snscrape) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from snscrape) (2.23.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from snscrape) (4.9.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (1.7.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.0.0.tar.gz (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 5.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.0.0-py3-none-any.whl size=193022 sha256=54cb9cc780cc1c931fd47804b746dcbca326429be65af3621c2fe4ccd7658cf5\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/29/4d/3cfe7452ac7d8d83b1930f8a6205c3c9649b24e80f9029fc38\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "989dea07",
      "metadata": {
        "id": "989dea07"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "from datetime import date, timedelta\n",
        "import re\n",
        "import string\n",
        "from emoji import replace_emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e1c64d78",
      "metadata": {
        "id": "e1c64d78"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#https://www.kaggle.com/code/ludovicocuoghi/twitter-sentiment-analysis-with-bert-roberta/notebook\n",
        "def strip_all_entities(text): \n",
        "    \"\"\"\n",
        "    Function to remove all next lines, tabs, links, mentions and non utf8/ascii characters as well as transofrming everything to lower case.\n",
        "    Input: Text (String)\n",
        "    Ouput_Striped Text (String)\n",
        "    \"\"\"\n",
        "    text = text.replace('\\r', '').replace('\\n', ' ').replace('\\n', ' ').lower() #remove \\n and \\r and lowercase\n",
        "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text) #remove links and mentions\n",
        "    text = re.sub(r'[^\\x00-\\x7f]',r'', text) #remove non utf8/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n",
        "    banned_list= string.punctuation + 'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§'\n",
        "    table = str.maketrans('', '', banned_list)\n",
        "    text = text.translate(table)\n",
        "    return text\n",
        "\n",
        "#clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol\n",
        "def clean_hashtags(tweet):\n",
        "    \"\"\"\n",
        "    Funciton to clean a tweet from all hastags.\n",
        "    \"\"\"\n",
        "    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet)) #remove last hashtags\n",
        "    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet)) #remove hashtags symbol from words in the middle of the sentence\n",
        "    return new_tweet2\n",
        "\n",
        "#Filter special characters such as & and $ present in some words\n",
        "def filter_chars(a):\n",
        "    \"\"\"\n",
        "    Remove & and $ from words\n",
        "    \"\"\"\n",
        "    sent = []\n",
        "    for word in a.split(' '):\n",
        "        if ('$' in word) | ('&' in word):\n",
        "            sent.append('')\n",
        "        else:\n",
        "            sent.append(word)\n",
        "    return ' '.join(sent)\n",
        "\n",
        "def remove_mult_spaces(text): # remove multiple spaces\n",
        "    \"\"\"\n",
        "    Function to remove multiple spaces\n",
        "    \"\"\"\n",
        "    return re.sub(\"\\s\\s+\" , \" \", text)\n",
        "\n",
        "##### https://zhuanlan.zhihu.com/p/342441381\n",
        "def scrape_tweets(words, start, end, clean=True, save=True):\n",
        "    \"\"\"\n",
        "    Scrapes Twitter and returns the collected tweets\n",
        "    words: list of strings; contains the words that one wants to scrape tweets for\n",
        "    start: string start date for the scraping; needs to be in YYYY-MM-DD format\n",
        "    end: string end date for the scraping; needs to be in YYY-MM-DD format\n",
        "    save: to append the scraped data to an existing csv\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "    for word in words:\n",
        "        scraped_tweets = sntwitter.TwitterSearchScraper(f\"{word} since:{start} until:{end}\").get_items()\n",
        "\n",
        "        sliced_scraped_tweets = itertools.islice(scraped_tweets, None)\n",
        "        tweets_df = pd.DataFrame(sliced_scraped_tweets)\n",
        "        tweets_df = tweets_df[[\"id\", \"content\", \"date\"]]\n",
        "\n",
        "        df = pd.concat([df, tweets_df])\n",
        "\n",
        "        df.drop_duplicates(subset='id',inplace=True)\n",
        "        \n",
        "    if clean:\n",
        "        df[\"content\"] = df[\"content\"].apply(replace_emoji)\n",
        "        df[\"content\"] = df[\"content\"].apply(strip_all_entities)\n",
        "        df[\"content\"] = df[\"content\"].apply(clean_hashtags)\n",
        "        df[\"content\"] = df[\"content\"].apply(filter_chars)\n",
        "        df[\"content\"] = df[\"content\"].apply(remove_mult_spaces)\n",
        "    \n",
        "    if save:\n",
        "        df.to_csv(\"NLP_test_file.csv\", mode=\"a\") #a for append  \n",
        "        \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "227b1e1d",
      "metadata": {
        "id": "227b1e1d"
      },
      "outputs": [],
      "source": [
        "def scraper(words, start, end, number_of_days=False, clean=True, save=True):\n",
        "    \"\"\"\n",
        "    words: list of strings to scrape Twitter for\n",
        "    start: string start date for the scraping; needs to be in YYYY-MM-DD format \n",
        "    end: string end date for the scraping; needs to be in YYY-MM-DD format, \n",
        "         not inclusive of the last day\n",
        "    number_of_days: alternative to end\n",
        "                    int, the number of days beginning from start one wants to scrape\n",
        "    \"\"\"\n",
        "    start_date = date.fromisoformat(start)\n",
        "    if number_of_days:\n",
        "        end_date = start_date + timedelta(number_of_days)\n",
        "    elif end:\n",
        "        end_date = date.fromisoformat(end)\n",
        "    else:\n",
        "        print(\"You must provide at least a valid end date or the number of days you want to scrape twitter\")\n",
        "    if len(words) == 0:\n",
        "        print(f\"You are not searching for any words, current words input is:\\n{words}\")\n",
        "        \n",
        "    for i in range((end_date-start_date).days):\n",
        "        print(f\"scraping day {start_date}\")\n",
        "        next_day = start_date + timedelta(1)\n",
        "        df = scrape_tweets(words, start_date.isoformat(), next_day.isoformat(), clean=clean, save=save)\n",
        "        start_date = next_day\n",
        "        \n",
        "    return df\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b1f06242",
      "metadata": {
        "id": "b1f06242"
      },
      "outputs": [],
      "source": [
        "#define your words, one of them should be in a tweet for the tweet to be downloaded\n",
        "words = [\"refugee\", \"asylum\", \"escapee\", \"stateless person\", \"fugitive\", \"UNHCR\", \n",
        "         \"migrant\", \"civil conflict\", \"war\", \"hunger\"] \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c811f912",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c811f912",
        "outputId": "ff0260a5-ba92-47a9-8f4a-065da28a8ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scraping day 2020-06-01\n"
          ]
        }
      ],
      "source": [
        "#scrape twitter for your defined words from the 1st June of 2022 to 2nd June of 2022\n",
        "d = scraper(words, \"2020-06-01\", \"2020-06-02\", clean=True, save=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "name": "Twitter_NLP.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}